{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import IPython.display as ipd\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, AveragePooling1D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# from livelossplot import PlotLossesKeras\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data from datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = 'D://emotion_recognition_data'\n",
    "TESS = os.path.join(main_path, \"data3/\")\n",
    "RAV = os.path.join(main_path, \"data/\")\n",
    "SAVEE = os.path.join(main_path, \"data2/\")\n",
    "\n",
    "dir_list = os.listdir(SAVEE)\n",
    "dir_list[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAVDESS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "emotion = []\n",
    "voc_channel = []\n",
    "full_path = []\n",
    "modality = []\n",
    "intensity = []\n",
    "actors = []\n",
    "phrase =[]\n",
    "\n",
    "for root, dirs, files in tqdm(os.walk(RAV)):\n",
    "    for file in files:\n",
    "        try:\n",
    "            #Load librosa array, obtain mfcss, store the file and the mfcss information in a new array\n",
    "            # X, sample_rate = librosa.load(os.path.join(root,file), res_type='kaiser_fast')\n",
    "            # mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
    "            # The instruction below converts the labels (from 1 to 8) to a series from 0 to 7\n",
    "            # This is because our predictor needs to start from 0 otherwise it will try to predict also 0.\n",
    "           \n",
    "            modal = int(file[1:2])\n",
    "            vchan = int(file[4:5])\n",
    "            lab = int(file[7:8])\n",
    "            ints = int(file[10:11])\n",
    "            phr = int(file[13:14])\n",
    "            act = int(file[19:20])\n",
    "            \n",
    "            # arr = mfccs, lab\n",
    "            # lst.append(arr)\n",
    "            \n",
    "            modality.append(modal)\n",
    "            voc_channel.append(vchan)\n",
    "            emotion.append(lab) #only labels\n",
    "            intensity.append(ints)\n",
    "            phrase.append(phr)\n",
    "            actors.append(act)\n",
    "            \n",
    "            full_path.append((root, file)) # only files\n",
    "          # If the file is not valid, skip it\n",
    "        except ValueError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised\n",
    "emotions_list = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']\n",
    "emotion_dict = {em[0]+1:em[1] for em in enumerate(emotions_list)}\n",
    "\n",
    "df = pd.DataFrame([emotion, voc_channel, modality, intensity, actors, phrase, full_path]).T\n",
    "df.columns = ['emotion', 'voc_channel', 'modality', 'intensity', 'actors', 'phrase', 'path']\n",
    "df['emotion'] = df['emotion'].map(emotion_dict)\n",
    "df['voc_channel'] = df['voc_channel'].map({1: 'speech', 2:'song'})\n",
    "df['modality'] = df['modality'].map({1: 'full AV', 2:'video only', 3:'audio only'})\n",
    "df['intensity'] = df['intensity'].map({1: 'normal', 2:'strong'})\n",
    "df['actors'] = df['actors'].apply(lambda x: 'female' if x%2 == 0 else 'male')\n",
    "df['phrase'] = df['phrase'].map({1: 'Kids are talking by the door', 2:'Dogs are sitting by the door'})\n",
    "df['path'] = df['path'].apply(lambda x: x[0] + '/' + x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data location for SAVEE\n",
    "dir_list = os.listdir(SAVEE)\n",
    "\n",
    "# parse the filename to get the emotions\n",
    "emotion=[]\n",
    "path = []\n",
    "for i in dir_list:\n",
    "    if i[-8:-6]=='_a':\n",
    "        emotion.append('angry_male')\n",
    "    elif i[-8:-6]=='_d':\n",
    "        emotion.append('disgust_male')\n",
    "    elif i[-8:-6]=='_f':\n",
    "        emotion.append('fear_male')\n",
    "    elif i[-8:-6]=='_h':\n",
    "        emotion.append('happy_male')\n",
    "    elif i[-8:-6]=='_n':\n",
    "        emotion.append('neutral_male')\n",
    "    elif i[-8:-6]=='sa':\n",
    "        emotion.append('sad_male')\n",
    "    elif i[-8:-6]=='su':\n",
    "        emotion.append('surprise_male')\n",
    "    else:\n",
    "        emotion.append('Unknown') \n",
    "    path.append(SAVEE + i)\n",
    "    \n",
    "# Now check out the label count distribution \n",
    "SAVEE_df = pd.DataFrame(emotion, columns = ['emotion_label'])\n",
    "SAVEE_df['source'] = 'SAVEE'\n",
    "SAVEE_df = pd.concat([SAVEE_df, pd.DataFrame(path, columns = ['path'])], axis = 1)\n",
    "SAVEE_df.emotion_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEE_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir(TESS)\n",
    "dir_list.sort()\n",
    "dir_list\n",
    "\n",
    "path = []\n",
    "emotion = []\n",
    "\n",
    "for i in dir_list:\n",
    "    fname = os.listdir(TESS + i)\n",
    "    for f in fname:\n",
    "        if i == 'OAF_angry' or i == 'YAF_angry':\n",
    "            emotion.append('angry_female')\n",
    "        elif i == 'OAF_disgust' or i == 'YAF_disgust':\n",
    "            emotion.append('disgust_female')\n",
    "        elif i == 'OAF_Fear' or i == 'YAF_fear':\n",
    "            emotion.append('fear_female')\n",
    "        elif i == 'OAF_happy' or i == 'YAF_happy':\n",
    "            emotion.append('happy_female')\n",
    "        elif i == 'OAF_neutral' or i == 'YAF_neutral':\n",
    "            emotion.append('neutral_female')                                \n",
    "        elif i == 'OAF_Pleasant_surprise' or i == 'YAF_pleasant_surprised':\n",
    "            emotion.append('surprise_female')               \n",
    "        elif i == 'OAF_Sad' or i == 'YAF_sad':\n",
    "            emotion.append('sad_female')\n",
    "        else:\n",
    "            emotion.append('Unknown')\n",
    "        path.append(TESS + i + \"/\" + f)\n",
    "\n",
    "TESS_df = pd.DataFrame(emotion, columns = ['emotion_label'])\n",
    "TESS_df['source'] = 'TESS'\n",
    "TESS_df = pd.concat([TESS_df,pd.DataFrame(path, columns = ['path'])],axis=1)\n",
    "TESS_df.emotion_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESS_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAV_df = df.copy()\n",
    "\n",
    "# only speech\n",
    "RAV_df = RAV_df.loc[RAV_df.voc_channel == 'speech']\n",
    "\n",
    "RAV_df.insert(0, \"emotion_label\", RAV_df.emotion+'_'+RAV_df.actors, True)\n",
    "RAV_df.insert(1, \"source\", \"RAV\", True)\n",
    "RAV_df = RAV_df.drop(['emotion', 'voc_channel', 'modality', 'intensity', 'phrase'], 1)\n",
    "RAV_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESS_df.insert(2, \"actors\", \"female\", True)\n",
    "SAVEE_df.insert(2, \"actors\", \"male\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEE_df['emotion_label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([TESS_df, RAV_df, SAVEE_df])\n",
    "df_combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['emotion_label', 'source', 'actors']:\n",
    "    print('\\nColumn values for ', col.upper())\n",
    "    print(df_combined[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels_dict_comb = {'angry_male':'negative_male', 'angry_female':'negative_female', \n",
    "                        'calm_male':'neutral_male', 'calm_female':'neutral_female',\n",
    "                        'disgust_male':'negative_male', 'disgust_female':'negative_female', \n",
    "                        'fearful_male':'negative_male','fearful_female':'negative_female',\n",
    "                        'fear_male':'negative_male', 'fear_female':'negative_female',\n",
    "                        'happy_male':'positive_male', 'happy_female':'positive_female',\n",
    "                        'neutral_male':'neutral_male', 'neutral_female':'neutral_female',\n",
    "                        'sad_male':'negative_male', 'sad_female':'negative_female',\n",
    "                        'surprised_male':'positive_male', 'surprised_female':'positive_female',\n",
    "                        'surprise_male':'positive_male', 'surprise_female':'positive_female',\n",
    "                        'Unknown': 'unk'}\n",
    "\n",
    "df_combined['emotion2'] = df_combined['emotion_label'].map(new_labels_dict_comb)\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addit_labels_dict_comb = {'angry_male':'angry', 'angry_female':'angry', \n",
    "                        'calm_male':'neutral', 'calm_female':'neutral',\n",
    "                        'disgust_male':'negative', 'disgust_female':'negative', \n",
    "                        'fearful_male':'fear','fearful_female':'fear',\n",
    "                        'fear_male':'fear', 'fear_female':'fear',\n",
    "                        'happy_male':'positive', 'happy_female':'positive',\n",
    "                        'neutral_male':'neutral', 'neutral_female':'neutral',\n",
    "                        'sad_male':'sadness', 'sad_female':'sadness',\n",
    "                        'surprised_male':'surprise', 'surprised_female':'surprise',\n",
    "                        'surprise_male':'surprise', 'surprise_female':'surprise',\n",
    "                        'Unknown': 'unk'}\n",
    "\n",
    "df_combined['emotion3'] = df_combined['emotion_label'].map(addit_labels_dict_comb)\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.emotion2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.emotion3.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = df_combined.loc[df_combined['emotion2'] != 'unk']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_csv(os.path.join(main_path,\"combined.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Using the combined DF, MFCC's and column 'emotion2' as target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "df_path = 'D://emotion_recognition_data//combined.csv'\n",
    "# mfccs_path = 'd://itc//final_project//mfccs.pickle'\n",
    "# new_y_path = 'd://itc//final_project//y.pickle'\n",
    "\n",
    "# with open('d://downloads//y.pickle', 'wb') as f:\n",
    "#     pickle.dump(new_y, f)\n",
    "    \n",
    "# with open('d://downloads//mfccs.pickle', 'wb') as f:\n",
    "#     pickle.dump(mfccs, f)\n",
    "\n",
    "mydf = pd.read_csv(df_path)\n",
    "\n",
    "# with open(mfccs_path, 'rb') as f:\n",
    "#     mfccs = pickle.load(f)\n",
    "\n",
    "# with open(new_y_path, 'rb') as f:\n",
    "#     new_y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.random.randint(0,len(mydf))\n",
    "data, sampling_rate = librosa.load(mydf['path'][ind], sr=44100)\n",
    "emotion = mydf['emotion2'][ind]\n",
    "\n",
    "plt.title(f'Sound wave of- {emotion}')\n",
    "librosa.display.waveplot(data, sampling_rate)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear-scale spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.abs(librosa.stft(data))\n",
    "librosa.display.specshow(D, sr=sampling_rate, x_axis='time', y_axis='linear');\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log-scale Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB = librosa.amplitude_to_db(D, ref=np.max)\n",
    "librosa.display.specshow(DB, sr=sampling_rate, x_axis='time', y_axis='log');\n",
    "plt.colorbar(format='%+2.0f db')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log-scale spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = plt.magnitude_spectrum(data, scale='dB')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.log(a[0])\n",
    "a2 = a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(a2,a1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hl_envelopes_idx(s, dmin=1, dmax=1, split=False):\n",
    "    \"\"\"\n",
    "    Input :\n",
    "    s: 1d-array, data signal from which to extract high and low envelopes\n",
    "    dmin, dmax: int, optional, size of chunks, use this if the size of the input signal is too big\n",
    "    split: bool, optional, if True, split the signal in half along its mean, might help to generate the envelope in some cases\n",
    "    Output :\n",
    "    lmin,lmax : high/low envelope idx of input signal s\n",
    "    \"\"\"\n",
    "\n",
    "    # locals min      \n",
    "    lmin = (np.diff(np.sign(np.diff(s))) > 0).nonzero()[0] + 1 \n",
    "    # locals max\n",
    "    lmax = (np.diff(np.sign(np.diff(s))) < 0).nonzero()[0] + 1 \n",
    "    \n",
    "\n",
    "    if split:\n",
    "        # s_mid is zero if s centered around x-axis or more generally mean of signal\n",
    "        s_mid = np.mean(s) \n",
    "        # pre-sorting of locals min based on relative position with respect to s_mid \n",
    "        lmin = lmin[s[lmin]<s_mid]\n",
    "        # pre-sorting of local max based on relative position with respect to s_mid \n",
    "        lmax = lmax[s[lmax]>s_mid]\n",
    "\n",
    "\n",
    "    # global max of dmax-chunks of locals max \n",
    "    lmin = lmin[[i+np.argmin(s[lmin[i:i+dmin]]) for i in range(0,len(lmin),dmin)]]\n",
    "    # global min of dmin-chunks of locals min \n",
    "    lmax = lmax[[i+np.argmax(s[lmax[i:i+dmax]]) for i in range(0,len(lmax),dmax)]]\n",
    "    \n",
    "    return lmin,lmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_idx, _ = hl_envelopes_idx(data[:len(a1)], dmin=250)\n",
    "\n",
    "# plot\n",
    "plt.plot(a2[high_idx], a1[high_idx], 'b', label='low')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DF from MFCC's and 'emotion2' columns as labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y = mydf['emotion2'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for i in tqdm(mydf['path']):\n",
    "    X.append(librosa.load(i, res_type='kaiser_fast', sr=44000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = []\n",
    "for ind,i in enumerate(X):\n",
    "    new_x.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(x) for x in new_x]\n",
    "\n",
    "plt.title('Lengths distribution')\n",
    "sns.boxplot(lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 300000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = np.array(lengths)\n",
    "print((lengths > thresh).sum())\n",
    "new_lengths = lengths[lengths < thresh]\n",
    "\n",
    "sns.boxplot(new_lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lengths.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_chosen = 120378"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "X_new = []\n",
    "y_new = []\n",
    "for ind,i in enumerate(new_x):\n",
    "    if i.shape[0] < 300000:\n",
    "        if i.shape[0] > length_chosen:\n",
    "            new = i[:length_chosen]\n",
    "            X_new.append(new)\n",
    "        elif i.shape[0] < length_chosen:\n",
    "            new = np.pad(i,math.ceil((length_chosen-i.shape[0])/2), mode='median')\n",
    "            X_new.append(new)\n",
    "        else:\n",
    "            X_new.append(i)\n",
    "            \n",
    "        y_new.append(new_y[ind])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X_new)\n",
    "y = np.array(y_new)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = []\n",
    "for i in tqdm(X):\n",
    "    mfcc = librosa.feature.mfcc(y=i, sr=44000, n_mfcc=20)\n",
    "    mfcc = mfcc.T\n",
    "    mfccs.append(mfcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = np.array(mfccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf.shape, mfccs.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs_path = 'd://ITC//final_project//mfccs.pickle'\n",
    "y_path = 'd://ITC//final_project//y.pickle'\n",
    "\n",
    "with open(mfccs_path, 'wb') as f:\n",
    "    pickle.dump(mfccs,f)\n",
    "    \n",
    "with open(y_path, 'wb') as f:\n",
    "    pickle.dump(y,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASELINE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At this point, we can perform a normal classification, using our mfccs coefficients as our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random:\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(mfccs, y, test_size=0.20)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### We'll change the values in our target variable and expand the dimension of our features to fit the neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "     layers.Conv1D(64, 3, activation='relu', input_shape=(236,20)),\n",
    "     layers.MaxPooling1D(),\n",
    "     layers.Conv1D(64, 3, activation='relu'),\n",
    "     layers.MaxPooling1D(),\n",
    "     layers.Conv1D(64, 3, activation='relu'),\n",
    "     layers.MaxPooling1D(),\n",
    "     layers.Flatten(),\n",
    "     layers.Dense(64, activation=\"relu\"),\n",
    "     layers.Dense(6, activation=\"softmax\")\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.input_shape, model.output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_encode = {'negative_female':0, 'negative_male':1, 'neutral_female':2, 'neutral_male':3,\n",
    "                  'positive_female':4, 'positive_male':5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.Series(y_train).map(emotions_encode)\n",
    "y_test = pd.Series(y_test).map(emotions_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics='accuracy')\n",
    "model.fit(X_train, y_train, batch_size=16, epochs=50, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = [np.argmax(x) for x in y_pred]\n",
    "print(classification_report(y_test, y_pred, target_names = list(emotions_encode.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf['emotion2'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "model2.add(layers.Conv1D(256, 5,padding='same',\n",
    "                 input_shape=(236,20)))\n",
    "model2.add(layers.Activation('relu'))\n",
    "model2.add(layers.MaxPooling1D(pool_size=(8)))\n",
    "model2.add(layers.Dropout(0.1))\n",
    "\n",
    "model2.add(layers.Conv1D(128, 5,padding='same'))\n",
    "model2.add(layers.Activation('relu'))\n",
    "model2.add(layers.Dropout(0.1))\n",
    "\n",
    "model2.add(layers.Flatten())\n",
    "model2.add(layers.Dense(6))\n",
    "model2.add(layers.Activation('softmax'))\n",
    "\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                                 factor=0.5, patience=4, \n",
    "                                                 verbose=1, mode='max', \n",
    "                                                 min_lr=0.00001)\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=40, \n",
    "                                              verbose=1)\n",
    "\n",
    "weight_path = 'd://ITC//final_project//best_weights.hdf5'\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=weight_path, \n",
    "                                                      save_weights_only=True, \n",
    "                                                      monitor='val_accuracy', \n",
    "                                                      mode='max', \n",
    "                                                      save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics='accuracy')\n",
    "model2.fit(X_train, y_train, batch_size=16, epochs=500, validation_data=(X_test, y_test),\n",
    "           callbacks=[reduce_lr, early_stop, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.load_weights('d://ITC//final_project//best_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model2.predict(X_test)\n",
    "y_pred = [np.argmax(x) for x in y_pred]\n",
    "print(classification_report(y_test, y_pred, target_names = list(emotions_encode.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiy emotions- first we'll need to create new MFCC's and target arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf['emotion3'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mydf['emotion3'].replace(['fear_female', 'fear_male'], 'fear', inplace=True)\n",
    "# mydf['emotion3'].replace(['surprise_female', 'surprise_male'], 'surprise', inplace=True)\n",
    "# mydf['emotion3'].replace(['sad_female', 'sad_male'], 'sadness', inplace=True)\n",
    "# mydf['emotion3'].replace(['negative_female', 'negative_male'], 'negetive', inplace=True)\n",
    "# mydf['emotion3'].replace(['positive_female', 'positive_male'], 'positive', inplace=True)\n",
    "# mydf['emotion3'].replace(['neutral_female', 'neutral_male'], 'neutral', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Emotions distribution')\n",
    "plt.hist(mydf['emotion3'])\n",
    "# plt.hist(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = mydf['emotion3'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for i in tqdm(mydf['path']):\n",
    "    X.append(librosa.load(i, res_type='kaiser_fast', sr=44000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = []\n",
    "for ind,i in enumerate(X):\n",
    "    new_x.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(x) for x in new_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = np.array(lengths)\n",
    "print((lengths > 300000).sum())\n",
    "new_lengths = lengths[lengths < 300000]\n",
    "\n",
    "sns.boxplot(new_lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_lengths.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_chosen = 120378"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "X_new = []\n",
    "y_new = []\n",
    "for ind,i in enumerate(new_x):\n",
    "    if i.shape[0] < 300000:\n",
    "        if i.shape[0] > length_chosen:\n",
    "            new = i[:length_chosen]\n",
    "            X_new.append(new)\n",
    "        elif i.shape[0] < length_chosen:\n",
    "            new = np.pad(i,math.ceil((length_chosen-i.shape[0])/2), mode='median')\n",
    "            X_new.append(new)\n",
    "        else:\n",
    "            X_new.append(i)\n",
    "            \n",
    "        y_new.append(y[ind])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X_new)\n",
    "y = np.array(y_new)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = []\n",
    "for i in tqdm(X):\n",
    "    mfcc = librosa.feature.mfcc(y=i, sr=44000, n_mfcc=40)\n",
    "    mfcc = mfcc.T\n",
    "    mfccs.append(mfcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = np.array(mfccs)\n",
    "mfccs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "mfccs2_path = 'D://emotion_recognition_data//mfccs2.pickle'\n",
    "y2_path = 'D://emotion_recognition_data//y2.pickle'\n",
    "\n",
    "# with open(mfccs2_path, 'wb') as f:\n",
    "#     pickle.dump(mfccs,f)\n",
    "    \n",
    "# with open(y2_path, 'wb') as f:\n",
    "#     pickle.dump(y,f)\n",
    "    \n",
    "with open(mfccs2_path, 'rb') as f:\n",
    "    mfccs = pickle.load(f)\n",
    "    \n",
    "with open(y2_path, 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.where(y=='positive', 'happy', y)\n",
    "y = np.where(y=='negative', 'disgust', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_enc = {'fear':0, 'disgust':1, 'neutral':2, 'happy':3, 'sadness':4, 'surprise':5, 'angry':6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(mfccs, y, test_size=0.2, random_state=12)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=15)\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.Series(y_train).map(emotion_enc)\n",
    "y_val = pd.Series(y_val).map(emotion_enc)\n",
    "y_test = pd.Series(y_test).map(emotion_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.unique(), y_test.unique(), y_val.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "\n",
    "model3.add(layers.Conv1D(256, 5,padding='same',\n",
    "                 input_shape=(236,40)))\n",
    "model3.add(layers.Activation('relu'))\n",
    "model3.add(layers.MaxPooling1D(pool_size=(8)))\n",
    "model3.add(layers.Dropout(0.2))\n",
    "\n",
    "model3.add(layers.Conv1D(128, 5,padding='same'))\n",
    "model3.add(layers.Activation('relu'))\n",
    "model3.add(layers.MaxPooling1D(pool_size=(4)))\n",
    "model3.add(layers.Dropout(0.1))\n",
    "\n",
    "model3.add(layers.Flatten())\n",
    "model3.add(layers.Dense(64))\n",
    "model3.add(layers.Dense(7))\n",
    "model3.add(layers.Activation('softmax'))\n",
    "\n",
    "model3.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_path2 = 'd://ITC//final_project_data//best_weights3.hdf5'\n",
    "weight_path2 = 'D://emotion_recognition_data//best_weights2.hdf5'\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                                 factor=0.5, patience=4, \n",
    "                                                 verbose=1, mode='max', \n",
    "                                                 min_lr=0.00001)\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=45, \n",
    "                                              verbose=1)\n",
    "\n",
    "model_checkpoint2 = tf.keras.callbacks.ModelCheckpoint(filepath=weight_path2, \n",
    "                                                      save_weights_only=True, \n",
    "                                                      monitor='val_accuracy', \n",
    "                                                      mode='max', \n",
    "                                                      save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model3.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics='accuracy')\n",
    "model3.fit(X_train, y_train, batch_size=16, epochs=500, validation_data=(X_val, y_val),\n",
    "           callbacks=[reduce_lr, early_stop, model_checkpoint2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model3.load_weights(weight_path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_model = tf.keras.applications.DenseNet201(include_top=False, \n",
    "#                                                      weights='imagenet', \n",
    "#                                                      input_shape=(236,40,3))\n",
    "# # pretrained_model.trainable = False\n",
    "# for layer in pretrained_model.layers:\n",
    "#   if 'conv5' in layer.name:\n",
    "#     layer.trainable = True\n",
    "#   else:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# pretrained_model.input_shape, pretrained_model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_expand = np.expand_dims(X_train, 3)\n",
    "# X_test_expand = np.expand_dims(X_test, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_expand.shape, X_test_expand.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before_model = Sequential()\n",
    "\n",
    "# before_model.add(layers.Conv1D(256, 5,padding='same',\n",
    "#                  input_shape=(236,40,1)))\n",
    "# before_model.add(layers.Activation('relu'))\n",
    "# before_model.add(layers.Dropout(0.2))\n",
    "# # before_model.add(layers.UpSampling2D(size=2))\n",
    "# before_model.add(layers.Dense(64))\n",
    "# before_model.add(layers.Dense(3))\n",
    "\n",
    "# before_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after_model = tf.keras.models.Sequential()\n",
    "# after_model.add(before_model)\n",
    "# after_model.add(pretrained_model)\n",
    "# after_model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "# after_model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# after_model.add(tf.keras.layers.Dense(256))\n",
    "# after_model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "# after_model.add(tf.keras.layers.Dense(128))\n",
    "# after_model.add(tf.keras.layers.Dropout(0.1))\n",
    "# after_model.add(tf.keras.layers.Dense(6, activation='softmax'))\n",
    "\n",
    "# after_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics='accuracy')\n",
    "# after_model.fit(X_train_expand, y_train, batch_size=32, epochs=500, validation_data=(X_test_expand, y_test),\n",
    "#            callbacks=[reduce_lr, early_stop, model_checkpoint2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_res_and_plot_matrix(y_test, y_pred, plot_classes):\n",
    "\n",
    "  #report metrics\n",
    "  acc = accuracy_score(y_test, y_pred)\n",
    "  print(f\"Accuracy: {acc:.4f}\")\n",
    "  # print(f\"Classes: {plot_classes}\")\n",
    "\n",
    "  #plot matrix\n",
    "  cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "  fig, ax = plt.subplots()\n",
    "    \n",
    "  tick_marks = np.arange(len(plot_classes))\n",
    "  plt.xticks(ticks=tick_marks, labels=plot_classes, rotation=90)\n",
    "  plt.yticks(ticks=tick_marks, labels=plot_classes, rotation=90)\n",
    "\n",
    "  group_counts = [f'{value:0.0f}' for value in cnf_matrix.flatten()]\n",
    "  group_percentages = [f'{100 * value:0.1f} %' for value in \n",
    "                       cnf_matrix.flatten()/np.sum(cnf_matrix)]\n",
    "  labels = [f'{v1}\\n({v2})' for v1, v2 in\n",
    "            zip(group_counts,group_percentages)]\n",
    "  n = int(np.sqrt(len(labels)))\n",
    "  labels = np.asarray(labels).reshape(n,n)\n",
    "  sns.heatmap(cnf_matrix, annot=labels, fmt='', cmap='Blues',\n",
    "              xticklabels=plot_classes, yticklabels=plot_classes)\n",
    "\n",
    "\n",
    "  ax.xaxis.set_label_position(\"bottom\")\n",
    "  plt.tight_layout()\n",
    "  plt.title('Confusion matrix', y=1.1)\n",
    "  plt.ylabel('Actual label')\n",
    "  plt.xlabel('Predicted label')\n",
    "  plt.show()\n",
    "\n",
    "  # return metrics\n",
    "  return [acc, cnf_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model3.predict(X_test).argmax(axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=list(emotion_enc.keys())))\n",
    "params = report_res_and_plot_matrix(y_test, y_pred, list(emotion_enc.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of our classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape, X_train.shape, y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_new = X_test.reshape(472,-1).copy()\n",
    "X_train_new = X_train.reshape(3773,-1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_new.shape, X_train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_new)\n",
    "X_train_scaled = scaler.transform(X_train_new)\n",
    "X_test_scaled = scaler.transform(X_test_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit_transform(X_train_scaled)\n",
    "X_pca = pca.transform(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(X_pca)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figuring out which label resambles which class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = pd.get_dummies(labels)\n",
    "p = pd.get_dummies(y_pred)\n",
    "\n",
    "l.shape, p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = pd.merge(l, p, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = h.corr(method='spearman')\n",
    "mask = np.tril(np.ones_like(corr, dtype=np.bool))\n",
    "ax = plt.figure(figsize=(25,10))\n",
    "ax = sns.heatmap(h.corr(method='spearman'), annot=True, fmt=\".2f\", mask = mask, square = True, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(labels)\n",
    "b= pd.DataFrame(y_pred)\n",
    "c= pd.DataFrame()\n",
    "c['labels'] = a[0].copy()\n",
    "c['preds'] = b[0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "sns.countplot(x='labels', hue=\"preds\", data=c)\n",
    "plt.xlabel('Labels', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.legend(title='Classes', bbox_to_anchor = (1,1), labels=list(emotion_enc.keys()), fontsize='large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corresponding classes:\n",
    "\n",
    "- label 0 - class 5\n",
    "- label 1 - class 2\n",
    "- label 2 - class 1\n",
    "- label 3 - class 3\n",
    "- label 4 - class 6\n",
    "- label 5 - class 0\n",
    "- label 6 - class 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {0:'blue', 1:'red', 2:'green', 3:'orange', 4:'black', 5:'grey', 6:'brown'}\n",
    "lab = ['surprise', 'neutral', 'disgust', 'happy', 'angry', 'fear', 'sadness']\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (15, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(X_pca[:, 0], X_pca[:, 1], c = pd.Series(labels).map(colors), alpha=0.5)\n",
    "for i in range(7):\n",
    "    plt.scatter(None, None, color=colors[i], label=lab[i])\n",
    "\n",
    "plt.title('Emotions divided to clusters', fontsize=20)\n",
    "plt.legend(fontsize=15, bbox_to_anchor= [1, 1.05])\n",
    "plt.xlabel('PCA 1', fontsize=15)\n",
    "plt.ylabel('PCA 2', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# model2.save('d://ITC//final_project_data//model2.h5')\n",
    "model3.save('D://emotion_recognition_data//model3.h5')\n",
    "\n",
    "model3 = tf.keras.models.load_model('D://emotion_recognition_data//model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check random samples from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,sr = librosa.load('D://emotion_recognition_data//data//data//Actor_01//03-01-04-02-02-01-01.wav',\n",
    "             res_type='kaiser_fast', sr=44000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, length_chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if x.shape[0] > length_chosen:\n",
    "        new = x[:length_chosen]\n",
    "elif x.shape[0] < length_chosen:\n",
    "        new = np.pad(x,math.ceil((length_chosen-x.shape[0])/2), mode='median')\n",
    "else:\n",
    "        new = x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc = librosa.feature.mfcc(y=new, sr=44000, n_mfcc=40)\n",
    "mfcc = mfcc.T\n",
    "mfcc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc = mfcc.reshape(1,236,40)\n",
    "mfcc.shape\n",
    "p = model3.predict(mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
