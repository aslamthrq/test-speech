{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MeidanGR/SpeechEmotionRecognition_Realtime/blob/main/3_realtime_ser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2QAnJ2-D_G0"
      },
      "source": [
        "# **Speech Emotion Recognition (Classification) in real-time using Deep LSTM layers**\n",
        "### ***Real-time implementaion of the SER model***\n",
        "---\n",
        "\n",
        "### Final project (B.Sc. requirement)  \n",
        "Development by **Meidan Greenberg & Linoy Hadad.**\n",
        "\n",
        "Instructor: **Dr. Dima Alberg**\n",
        "\n",
        "Industial Engineering and Management dept.\n",
        "\n",
        "SCE Collage, Israel.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Gcq7z02GfSH"
      },
      "source": [
        "# **ABSTRACT**\n",
        "This implementation of the pre-developed speech emotion recognition (SER) model is intended to analyze a speech audio input in real-time, identify and present the expressed emotion within it.\n",
        "\n",
        "\n",
        "Classifying emotion from speech is a long-term problem rather than a short-term one. In Linguistics, two same phrases of speech can receive a different meaning just by emphasizing a word or even a syllable.\n",
        "\n",
        "The model, an 87.23% accuracy LSTM based deep learning network, had learned how to classify the correct emotion expressed in a long-time sequence of speech features extracted from the audio signal.\n",
        "\n",
        "This system will record audio input, create a temporary .wav file containing the audio signals recorded, preprocess it, and present the distribution of emotions found in speech. The sequence length chosen for the task is 7.1 seconds.\n",
        "The process is cyclic and continues to accrue as long as there is continuous speech. After a silence of 2 seconds or more at the end of a sentence, the process will cease. At the end of a session, a summary is presented, holding the mean values of all emotions recognized during the session."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGNQaFNUEQMm"
      },
      "source": [
        "# **LIBRARIES & GOOGLE AUTH**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVEzIMYrFpEf",
        "outputId": "7366d8fb-848a-4f7e-c976-3624260b14ab"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lA4vLC6ID715"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pydub\n",
        "!pip install noisereduce\n",
        "!pip install pyaudio\n",
        "!pip install json-tricks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4cVP0NmD71-"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "from json_tricks import load\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import librosa\n",
        "from pydub import AudioSegment, effects\n",
        "import noisereduce as nr\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import model_from_json\n",
        "from keras.models import load_model\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSjs9TReEUls"
      },
      "source": [
        "# **LOAD MODEL**\n",
        "Loading the speech emotion recognition LSTM model and weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6qXd1-vD72A",
        "outputId": "9a687758-6391-4ec5-ec83-e52bb0f5ffc9"
      },
      "outputs": [],
      "source": [
        "saved_model_path = '/content/drive/My Drive/Colab Notebooks/model8723.json'\n",
        "saved_weights_path = '/content/drive/My Drive/Colab Notebooks/model8723_weights.h5'\n",
        "\n",
        "#Reading the model from JSON file\n",
        "with open(saved_model_path, 'r') as json_file:\n",
        "    json_savedModel = json_file.read()\n",
        "    \n",
        "# Loading the model architecture, weights\n",
        "model = tf.keras.models.model_from_json(json_savedModel)\n",
        "model.load_weights(saved_weights_path)\n",
        "\n",
        "# Compiling the model with similar parameters as the original model.\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "                optimizer='RMSProp', \n",
        "                metrics=['categorical_accuracy'])\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIrA6eoQGMiH"
      },
      "source": [
        "# **DATA PREPROCESSING**\n",
        "\n",
        "An audio input .wav file is processed similarly to the model's preprocess in the following order:\n",
        "\n",
        "\n",
        "*   Sample rate: number of audio samples per second is extracted by `librosa`.\n",
        "*   'AudioSegment' instance: The audio is loaded to an object by the `AudioSegment` module of `pydub`.\n",
        "*   Normalization: The 'AudioSegment' object is normalized to + 5.0 dBFS, by `effects` module of `pydub`. \n",
        "*   Transforming the object to an array of samples by `numpy` & `AudioSegment`.\n",
        "*   Noise reduction is being performed by `noisereduce`.\n",
        "\n",
        "Speech features are extracted as well:\n",
        "1.   Energy - Root Mean Square (RMS)\n",
        "2.   Zero Crossed Rate (ZCR)\n",
        "3.   Mel-Frequency Cepstral Coefficients (MFCCs) \n",
        "\n",
        "With `frame_length = 2048`, `hop_lentgh = 512`, assuring equally sequential length. \n",
        "\n",
        "The features are concatenated to an '`X`' variable, adjusted to fit the expected shape of the model: (batch, timesteps, feature). The function returns '`X_3D`' variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyCwGZaeD72C"
      },
      "outputs": [],
      "source": [
        "def preprocess(file_path, frame_length = 2048, hop_length = 512):\n",
        "    '''\n",
        "    A process to an audio .wav file before execcuting a prediction.\n",
        "      Arguments:\n",
        "      - file_path - The system path to the audio file.\n",
        "      - frame_length - Length of the frame over which to compute the speech features. default: 2048\n",
        "      - hop_length - Number of samples to advance for each frame. default: 512\n",
        "\n",
        "      Return:\n",
        "        'X_3D' variable, containing a shape of: (batch, timesteps, feature) for a single file (batch = 1).\n",
        "    ''' \n",
        "    # Fetch sample rate.\n",
        "    _, sr = librosa.load(path = file_path, sr = None)\n",
        "    # Load audio file\n",
        "    rawsound = AudioSegment.from_file(file_path, duration = None) \n",
        "    # Normalize to 5 dBFS \n",
        "    normalizedsound = effects.normalize(rawsound, headroom = 5.0) \n",
        "    # Transform the audio file to np.array of samples\n",
        "    normal_x = np.array(normalizedsound.get_array_of_samples(), dtype = 'float32') \n",
        "    # Noise reduction                  \n",
        "    final_x = nr.reduce_noise(normal_x, sr=sr, use_tensorflow=True)\n",
        "        \n",
        "        \n",
        "    f1 = librosa.feature.rms(final_x, frame_length=frame_length, hop_length=hop_length, center=True, pad_mode='reflect').T # Energy - Root Mean Square\n",
        "    f2 = librosa.feature.zero_crossing_rate(final_x, frame_length=frame_length, hop_length=hop_length,center=True).T # ZCR\n",
        "    f3 = librosa.feature.mfcc(final_x, sr=sr, S=None, n_mfcc=13, hop_length = hop_length).T # MFCC   \n",
        "    X = np.concatenate((f1, f2, f3), axis = 1)\n",
        "    \n",
        "    X_3D = np.expand_dims(X, axis=0)\n",
        "    \n",
        "    return X_3D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuiRFk30CDGs"
      },
      "source": [
        "# **ADDITIONAL SETUP**\n",
        "- An **emotion list** is defined to translate the model prediction output to a readable form.\n",
        "\n",
        "- `is_silent` function is executed as a boolean state if silence of sequential audio was found. `is_silent` returns `True` when the maximum signal within the sequence is less than the threshold value defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXfNZp8qD72D"
      },
      "outputs": [],
      "source": [
        "# Emotions list is created for a readable form of the model prediction.\n",
        "\n",
        "emotions = {\n",
        "    0 : 'neutral',\n",
        "    1 : 'calm',\n",
        "    2 : 'happy',\n",
        "    3 : 'sad',\n",
        "    4 : 'angry',\n",
        "    5 : 'fearful',\n",
        "    6 : 'disgust',\n",
        "    7 : 'suprised'   \n",
        "}\n",
        "emo_list = list(emotions.values())\n",
        "\n",
        "def is_silent(data):\n",
        "    # Returns 'True' if below the 'silent' threshold\n",
        "    return max(data) < 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nmp_ojL4Urt"
      },
      "source": [
        "# **REAL-TIME IMPLEMENTATION**\n",
        "\n",
        "This implementation of an LSTM Speech Emotion Recognition model carries out a real-time emotion prediction of an audio input, recorded from the soundcard of the platform.\n",
        "The process includes the following:\n",
        "\n",
        "\n",
        "1.   Session start, opening a connection with the input channel by `pyaudio`.\n",
        "2.   If **not silent**, the input signals will be recorded to a .wav file, by `pyaudio` and `wave`.\n",
        "\n",
        "    2.1  After 7.1 seconds, the recording will stumble in order to send the last .wav file to the rest of the process before start recording a new one.\n",
        "\n",
        "    2.2  The .wav file is preprocessed, in `preprocess` function.\n",
        "\n",
        "    2.3  `model.predict` is executed, an array of 8 emotion probabilities is returned. E.g. `predictions = [array([p_neutral, p_calm, p_happy, p_sad, p_angry, p_feaful, p_disgust, p_suprised], dtype=float32)]`\n",
        "\n",
        "    2.4   `predictions` are transformed to a compact representation (without 'array' and 'dtype' statements) and saved in a list.\n",
        "\n",
        "    2.5 A visualization of `predictions` is shown by `matplotlib`.\n",
        "\n",
        "3.   Else, if silence is identified within the last 2 seconds of a .wav file:\n",
        "\n",
        "    3.1  Break; end of the session; close connections.\n",
        "\n",
        "    3.2 Visualize a summary of the session: Mean value of the predictions.\n",
        "\n",
        "    3.3 State the overall session time.\n",
        "\n",
        "\n",
        "## **VARIABLES EXPLAINED**\n",
        "RATE = Sample rate = 24414 which is the sample rate of most of the model's train data.\n",
        "\n",
        "CHUNK = A batch of sequential samples to process at once. Similar to 'hop_length' by `librosa`, defined 512.\n",
        "\n",
        "FORMAT = Sampling size and format, 32bit as in the model.\n",
        "\n",
        "CHANNELS = 1 for mono, a standard of audio recording in PC / cellphones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4s4DyZckD72E",
        "outputId": "862921df-ad36-4b23-9532-81762f128793"
      },
      "outputs": [],
      "source": [
        "import pyaudio\n",
        "import wave\n",
        "from array import array\n",
        "import struct\n",
        "import time\n",
        "\n",
        "# Initialize variables\n",
        "RATE = 24414\n",
        "CHUNK = 512\n",
        "RECORD_SECONDS = 7.1\n",
        "\n",
        "FORMAT = pyaudio.paInt32\n",
        "CHANNELS = 1\n",
        "WAVE_OUTPUT_FILE = \"/content/drive/My Drive/Colab Notebooks/output.wav\"\n",
        "\n",
        "# Open an input channel\n",
        "p = pyaudio.PyAudio()\n",
        "stream = p.open(format=FORMAT,\n",
        "                channels=CHANNELS,\n",
        "                rate=RATE,\n",
        "                input=True,\n",
        "                frames_per_buffer=CHUNK)\n",
        "\n",
        "\n",
        "# Initialize a non-silent signals array to state \"True\" in the first 'while' iteration.\n",
        "data = array('h', np.random.randint(size = 512, low = 0, high = 500))\n",
        "\n",
        "# SESSION START\n",
        "print(\"** session started\")\n",
        "total_predictions = [] # A list for all predictions in the session.\n",
        "tic = time.perf_counter()\n",
        "\n",
        "while is_silent(data) == False:\n",
        "    print(\"* recording...\")\n",
        "    frames = [] \n",
        "    data = np.nan # Reset 'data' variable.\n",
        "\n",
        "    timesteps = int(RATE / CHUNK * RECORD_SECONDS) # => 339\n",
        "\n",
        "    # Insert frames to 'output.wav'.\n",
        "    for i in range(0, timesteps):\n",
        "        data = array('l', stream.read(CHUNK)) \n",
        "        frames.append(data)\n",
        "\n",
        "        wf = wave.open(WAVE_OUTPUT_FILE, 'wb')\n",
        "        wf.setnchannels(CHANNELS)\n",
        "        wf.setsampwidth(p.get_sample_size(FORMAT))\n",
        "        wf.setframerate(RATE)\n",
        "        wf.writeframes(b''.join(frames))\n",
        "\n",
        "    print(\"* done recording\")\n",
        "\n",
        "    x = preprocess(WAVE_OUTPUT_FILE) # 'output.wav' file preprocessing.\n",
        "    # Model's prediction => an 8 emotion probabilities array.\n",
        "    predictions = model.predict(x, use_multiprocessing=True)\n",
        "    pred_list = list(predictions)\n",
        "    pred_np = np.squeeze(np.array(pred_list).tolist(), axis=0) # Get rid of 'array' & 'dtype' statments.\n",
        "    total_predictions.append(pred_np)\n",
        "    \n",
        "    # Present emotion distribution for a sequence (7.1 secs).\n",
        "    fig = plt.figure(figsize = (10, 2))\n",
        "    plt.bar(emo_list, pred_np, color = 'darkturquoise')\n",
        "    plt.ylabel(\"Probabilty (%)\")\n",
        "    plt.show()\n",
        "    \n",
        "    max_emo = np.argmax(predictions)\n",
        "    print('max emotion:', emotions.get(max_emo,-1))\n",
        "    \n",
        "    print(100*'-')\n",
        "    \n",
        "    # Define the last 2 seconds sequence.\n",
        "    last_frames = np.array(struct.unpack(str(96 * CHUNK) + 'B' , np.stack(( frames[-1], frames[-2], frames[-3], frames[-4],\n",
        "                                                                            frames[-5], frames[-6], frames[-7], frames[-8],\n",
        "                                                                            frames[-9], frames[-10], frames[-11], frames[-12],\n",
        "                                                                            frames[-13], frames[-14], frames[-15], frames[-16],\n",
        "                                                                            frames[-17], frames[-18], frames[-19], frames[-20],\n",
        "                                                                            frames[-21], frames[-22], frames[-23], frames[-24]),\n",
        "                                                                            axis =0)) , dtype = 'b')\n",
        "    if is_silent(last_frames): # If the last 2 seconds are silent, end the session.\n",
        "        break\n",
        "\n",
        "# SESSION END        \n",
        "toc = time.perf_counter()\n",
        "stream.stop_stream()\n",
        "stream.close()\n",
        "p.terminate()\n",
        "wf.close()\n",
        "print('** session ended')\n",
        "\n",
        "# Present emotion distribution for the whole session.\n",
        "total_predictions_np =  np.mean(np.array(total_predictions).tolist(), axis=0)\n",
        "fig = plt.figure(figsize = (10, 5))\n",
        "plt.bar(emo_list, total_predictions_np, color = 'indigo')\n",
        "plt.ylabel(\"Mean probabilty (%)\")\n",
        "plt.title(\"Session Summary\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Emotions analyzed for: {(toc - tic):0.4f} seconds\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "3_realtime_ser.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
