{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4191d14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from json_tricks import dump, load\n",
    "\n",
    "from pydub import AudioSegment, effects\n",
    "\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d49cedbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion kind validation function for TESS database, due to emotions written within the file names.\n",
    "def find_emotion_T(name): \n",
    "        if('neutral' in name): return \"01\"\n",
    "        elif('happy' in name): return \"03\"\n",
    "        elif('sad' in name): return \"04\"\n",
    "        elif('angry' in name): return \"05\"\n",
    "        elif('fear' in name): return \"06\"\n",
    "        elif('disgust' in name): return \"07\"\n",
    "        elif('ps' in name): return \"08\"\n",
    "        else: return \"-1\"\n",
    "        \n",
    "        \n",
    "# 'emotions' list fix for classification purposes:\n",
    "#     Classification values start from 0, Thus an 'n = n-1' operation has been executed for both RAVDESS and TESS databases:\n",
    "def emotionfix(e_num):\n",
    "    if e_num == \"01\":   return 0 # neutral\n",
    "    elif e_num == \"02\": return 1 # calm\n",
    "    elif e_num == \"03\": return 2 # happy\n",
    "    elif e_num == \"04\": return 3 # sad\n",
    "    elif e_num == \"05\": return 4 # angry\n",
    "    elif e_num == \"06\": return 5 # fear\n",
    "    elif e_num == \"07\": return 6 # disgust\n",
    "    else:               return 7 # suprised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9a442b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sample length: 204288\n"
     ]
    }
   ],
   "source": [
    "sample_lengths = []\n",
    "folder_path = './dataset'\n",
    "\n",
    "for subdir, dirs, files in os.walk(folder_path):\n",
    "  for file in files: \n",
    "    x, sr = librosa.load(path = os.path.join(subdir,file), sr = None)\n",
    "    xt, index = librosa.effects.trim(x, top_db=30)\n",
    "     \n",
    "    sample_lengths.append(len(xt))\n",
    "\n",
    "print('Maximum sample length:', np.max(sample_lengths))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ca56854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydub\\utils.py:198: RuntimeWarning: Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\n",
      "  warn(\"Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\", RuntimeWarning)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m       _, sr \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(subdir,file), sr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;66;03m# sr (the sample rate) is used for librosa's MFCCs. '_' is irrelevant.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Load the audio file.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m       rawsound \u001b[38;5;241m=\u001b[39m \u001b[43mAudioSegment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# Normalize the audio to +5.0 dBFS.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m       normalizedsound \u001b[38;5;241m=\u001b[39m effects\u001b[38;5;241m.\u001b[39mnormalize(rawsound, headroom \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5.0\u001b[39m) \n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydub\\audio_segment.py:728\u001b[0m, in \u001b[0;36mAudioSegment.from_file\u001b[1;34m(cls, file, format, codec, parameters, start_second, duration, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 728\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[43mmediainfo_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_ahead_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_ahead_limit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[0;32m    730\u001b[0m     audio_streams \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstreams\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    731\u001b[0m                      \u001b[38;5;28;01mif\u001b[39;00m x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcodec_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydub\\utils.py:274\u001b[0m, in \u001b[0;36mmediainfo_json\u001b[1;34m(filepath, read_ahead_limit)\u001b[0m\n\u001b[0;32m    271\u001b[0m         file\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    273\u001b[0m command \u001b[38;5;241m=\u001b[39m [prober, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-of\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m command_args\n\u001b[1;32m--> 274\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstdin_parameter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m output, stderr \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mcommunicate(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mstdin_data)\n\u001b[0;32m    276\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[0;32m   1022\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[0;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1034\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:1538\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1538\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1540\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1546\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1547\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1548\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1554\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1555\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import noisereduce as nr\n",
    "tic = time.perf_counter()\n",
    "\n",
    "# Initialize data lists\n",
    "rms = []\n",
    "zcr = []\n",
    "mfcc = []\n",
    "emotions = []\n",
    "fileName = []\n",
    "\n",
    "# Initialize variables\n",
    "total_length = 204288 # desired frame length for all of the audio samples.\n",
    "frame_length = 2048\n",
    "hop_length = 512\n",
    "\n",
    "folder_path = './dataset' \n",
    "\n",
    "for subdir, dirs, files in os.walk(folder_path):\n",
    "  for file in files: \n",
    "\n",
    "    # Fetch the sample rate.\n",
    "      _, sr = librosa.load(path = os.path.join(subdir,file), sr = None) # sr (the sample rate) is used for librosa's MFCCs. '_' is irrelevant.\n",
    "        \n",
    "# Load the audio file.\n",
    "      rawsound = AudioSegment.from_file(os.path.join(subdir,file)) \n",
    "    # Normalize the audio to +5.0 dBFS.\n",
    "      normalizedsound = effects.normalize(rawsound, headroom = 5.0) \n",
    "    # Transform the normalized audio to np.array of samples.\n",
    "      normal_x = np.array(normalizedsound.get_array_of_samples(), dtype = 'float32')\n",
    "    # Trim silence from the beginning and the end.\n",
    "      xt, index = librosa.effects.trim(normal_x, top_db=30)\n",
    "      #print(file,\"\\t\", len(xt), \"\\t\", rawsound.dBFS, \"\\t\", normalizedsound.dBFS) #--QA purposes if needed-- \n",
    "    # Pad for duration equalization.\n",
    "      padded_x = np.pad(xt, (0, total_length-len(xt)), 'constant')\n",
    "    # Noise reduction.\n",
    "      final_x = nr.reduce_noise(y=padded_x,\n",
    "                          sr=sr)\n",
    "       \n",
    "   # Features extraction \n",
    "      f1 = librosa.feature.rms(y=final_x, frame_length=frame_length, hop_length=hop_length) # Energy - Root Mean Square   \n",
    "      f2 = librosa.feature.zero_crossing_rate(y=final_x , frame_length=frame_length, hop_length=hop_length, center=True) # ZCR      \n",
    "      f3 = librosa.feature.mfcc(y=final_x, sr=sr, n_mfcc=13, hop_length = hop_length) # MFCC\n",
    "      \n",
    "   # Emotion extraction from the different databases\n",
    "      if (find_emotion_T(file) != \"-1\"): #TESS database validation\n",
    "            name = find_emotion_T(file)\n",
    "      else:                              #RAVDESS database validation\n",
    "            name = file[6:8]                      \n",
    "\n",
    "   # Filling the data lists  \n",
    "      rms.append(f1)\n",
    "      zcr.append(f2)\n",
    "      mfcc.append(f3)\n",
    "      fileName.append(file)\n",
    "      #print(file, emotionfix(name))\n",
    "      emotions.append(emotionfix(name)) \n",
    "\n",
    "toc = time.perf_counter()\n",
    "print(f\"Running time: {(toc - tic)/60:0.4f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42f3328",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f_rms = np.asarray(rms).astype('float32')\n",
    "f_rms = np.swapaxes(f_rms,1,2)\n",
    "f_zcr = np.asarray(zcr).astype('float32')\n",
    "f_zcr = np.swapaxes(f_zcr,1,2)\n",
    "f_mfccs = np.asarray(mfcc).astype('float32')\n",
    "f_mfccs = np.swapaxes(f_mfccs,1,2)\n",
    "\n",
    "print('ZCR shape:',f_zcr.shape)\n",
    "print('RMS shape:',f_rms.shape)\n",
    "print('MFCCs shape:',f_mfccs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99af3fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating all features to 'X' variable.\n",
    "X = np.concatenate((f_zcr, f_rms, f_mfccs), axis=2)\n",
    "\n",
    "# Preparing 'Y' as a 2D shaped variable.\n",
    "Y = np.asarray(emotions).astype('int8')\n",
    "Y = np.expand_dims(Y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919d29c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to train, validation, and test sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_tosplit, y_train, y_tosplit = train_test_split(X, Y, test_size = 0.224, random_state = 1,shuffle=True)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_tosplit, y_tosplit, test_size = 0.304, random_state = 1,shuffle=True)\n",
    "\n",
    "#'One-hot' vectors for Y: emotion classification\n",
    "y_train_class = tf.keras.utils.to_categorical(y_train, 8, dtype = 'int8')\n",
    "y_val_class = tf.keras.utils.to_categorical(y_val, 8, dtype = 'int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb0f882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, x_val, and x_test shape check.\n",
    "print(np.shape(x_train))\n",
    "print(np.shape(x_val))\n",
    "print(np.shape(x_test))\n",
    "print(np.shape(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9726a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from json_tricks import load\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import librosa\n",
    "from pydub import AudioSegment, effects\n",
    "import noisereduce as nr\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5173e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path  = f'./model.json'\n",
    "saved_weights_path = f'./model_weights.h5'\n",
    "#Reading the model from JSON file\n",
    "with open(saved_model_path, 'r') as json_file:\n",
    "    json_savedModel = json_file.read()\n",
    "    \n",
    "# Loading the model architecture, weights\n",
    "model = tf.keras.models.model_from_json(json_savedModel)\n",
    "model.load_weights(saved_weights_path)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)   \n",
    "# Compiling the model with similar parameters as the original model.\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "                optimizer=opt, \n",
    "                metrics=['categorical_accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fef6139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation score\n",
    "print(\"Evaluate data size : \" , np.shape(x_val)[0])\n",
    "loss,acc = model.evaluate(x_val, y_val_class, verbose=2)\n",
    "print(\"Accuracy\", str(acc * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a54fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2de8369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Confusion matrix\n",
    "import pandas as pd\n",
    "y_val_class = np.argmax(y_val_class, axis=1)\n",
    "predictions = model.predict(x_val)\n",
    "y_pred_class = np.argmax(predictions, axis=1)\n",
    "\n",
    "cm=confusion_matrix(y_val_class, y_pred_class)\n",
    "\n",
    "index = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']  \n",
    "columns = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']  \n",
    " \n",
    "cm_df = pd.DataFrame(cm,index,columns)                      \n",
    "plt.figure(figsize=(12,8))\n",
    "ax = plt.axes()\n",
    "\n",
    "sns.heatmap(cm_df, ax = ax, cmap = 'PuBu', fmt=\"d\", annot=True)\n",
    "ax.set_ylabel('True emotion')\n",
    "ax.set_xlabel('Predicted emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9fb137",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = cm.diagonal()\n",
    "row_sum = np.sum(cm,axis=1)\n",
    "acc = values / row_sum\n",
    "\n",
    "print('Validation set predicted emotions accuracy:')\n",
    "for e in range(0, len(values)):\n",
    "    print(index[e],':', f\"{(acc[e]):0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0bd8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_val[0][3],y_val_class[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1945d7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_val, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b208363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93417a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa212c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(predictions[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7808933",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = {\n",
    "    0 : 'neutral',\n",
    "    1 : 'calm',\n",
    "    2 : 'happy',\n",
    "    3 : 'sad', #SAD\n",
    "    4 : 'angry',\n",
    "    5 : 'fearful',\n",
    "    6 : 'disgust',\n",
    "    7 : 'suprised'   \n",
    "}\n",
    "emo_list = list(emotions.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28871a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path, subdir):\n",
    "    rms_t = []\n",
    "    zcr_t = []\n",
    "    mfcc_t = []\n",
    "    \n",
    "    _, sr = librosa.load(path = os.path.join(subdir,file_path), sr = None) # sr (the sample rate) is used for librosa's MFCCs. '_' is irrelevant.\n",
    "    # Load the audio file.\n",
    "    rawsound = AudioSegment.from_file( os.path.join(subdir,file_path)) \n",
    "    # Normalize the audio to +5.0 dBFS.\n",
    "    normalizedsound = effects.normalize(rawsound, headroom = 5.0) \n",
    "    # Transform the normalized audio to np.array of samples.\n",
    "    normal_x = np.array(normalizedsound.get_array_of_samples(), dtype = 'float32')\n",
    "    # Trim silence from the beginning and the end.\n",
    "    xt, index = librosa.effects.trim(normal_x, top_db=30)\n",
    "      #print(file,\"\\t\", len(xt), \"\\t\", rawsound.dBFS, \"\\t\", normalizedsound.dBFS) #--QA purposes if needed-- \n",
    "    # Pad for duration equalization.\n",
    "    padded_x = np.pad(xt, (0, total_length-len(xt)), 'constant')\n",
    "    # Noise reduction.\n",
    "    final_x = nr.reduce_noise(y=padded_x,\n",
    "                          sr=sr)\n",
    " \n",
    "   # Features extraction \n",
    "    f1 = librosa.feature.rms(y=final_x, frame_length=frame_length, hop_length=hop_length) # Energy - Root Mean Square   \n",
    "    f2 = librosa.feature.zero_crossing_rate(y=final_x , frame_length=frame_length, hop_length=hop_length, center=True) # ZCR      \n",
    "    f3 = librosa.feature.mfcc(y=final_x, sr=sr, n_mfcc=13, hop_length = hop_length) # MFCC\n",
    "      \n",
    "   # Emotion extraction from the different databases\n",
    "    if (find_emotion_T(file_path) != \"-1\"): #TESS database validation\n",
    "        name = find_emotion_T(file_path)\n",
    "    else:                              #RAVDESS database validation\n",
    "        name = file_path[6:8]  \n",
    "\n",
    "   # Filling the data lists  \n",
    "    rms_t.append(f1)\n",
    "    zcr_t.append(f2)\n",
    "    mfcc_t.append(f3)\n",
    "    \n",
    "    f_rms_t = np.asarray(rms_t).astype('float32')\n",
    "    f_rms_t = np.swapaxes(f_rms_t,1,2)\n",
    "    f_zcr_t = np.asarray(zcr_t).astype('float32')\n",
    "    f_zcr_t = np.swapaxes(f_zcr_t,1,2)\n",
    "    f_mfccs_t = np.asarray(mfcc_t).astype('float32')\n",
    "    f_mfccs_t = np.swapaxes(f_mfccs_t,1,2)\n",
    "\n",
    "    #print(file, emotionfix(name))\n",
    "\n",
    "\n",
    "    X = np.concatenate((f_zcr_t, f_rms_t, f_mfccs_t), axis=2)\n",
    "\n",
    "    predictions = model.predict(X, use_multiprocessing=True)\n",
    "\n",
    "    y_pred_class = np.argmax(predictions)\n",
    "    print(\"file : \" + str(file_path), \",Real emotion : \" + str(emotionfix(name)) + \" \" + emotions.get(emotionfix(name),-1), \",predictions : \" + str(y_pred_class)+ \" \" +emotions.get(y_pred_class,-1))\n",
    "    return y_pred_class\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd94386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = preprocess('03-01-01-01-01-01-02.wav','./dataset/Actor_02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc680202",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b29be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = './dataset' \n",
    "\n",
    "for subdir, dirs, files in os.walk(folder_path):\n",
    "  for file in files: \n",
    "    b = preprocess(file,subdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3844581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "from array import array\n",
    "import struct\n",
    "import time\n",
    "from datetime import datetime\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d7cdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voice_predictions():\n",
    "    \n",
    "    \n",
    "    RATE = 24414\n",
    "    CHUNK = 512\n",
    "    RECORD_SECONDS = 7\n",
    "    FORMAT = pyaudio.paInt32\n",
    "    CHANNELS = 1\n",
    "    WAVE_OUTPUT_FILE = \"./output_testing.wav\"\n",
    "    WAVE_OUTPUT_FILENAME_TESTING = \"./dataset/Actor_02/03-01-01-01-01-01-02.wav\"\n",
    "    \n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                frames_per_buffer=CHUNK)\n",
    "    \n",
    "    data = array('h', np.random.randint(size = 512, low = 0, high = 500))\n",
    "    \n",
    "    print(\"** Analysis started\")\n",
    "    total_predictions = [] # A list for all predictions in the session.\n",
    "    tic = time.perf_counter()\n",
    "    \n",
    "\n",
    "    print(\"* Analysis...\")\n",
    "    frames = [] \n",
    "    data = np.nan # Reset 'data' variable.\n",
    "\n",
    "    timesteps = int(RATE / CHUNK * RECORD_SECONDS) # => 339\n",
    "\n",
    "    # Insert frames to 'output.wav'.\n",
    "    for i in range(0, timesteps):\n",
    "        data = array('l', stream.read(CHUNK)) \n",
    "        frames.append(data)\n",
    "\n",
    "        wf = wave.open(WAVE_OUTPUT_FILE, 'wb')\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "        wf.setframerate(RATE)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "\n",
    "    print(\"* done Analysis\")\n",
    "\n",
    "    rms_t = []\n",
    "    zcr_t = []\n",
    "    mfcc_t = []\n",
    "    \n",
    "    _, sr = librosa.load(path = WAVE_OUTPUT_FILENAME_TESTING, sr = None) # sr (the sample rate) is used for librosa's MFCCs. '_' is irrelevant.\n",
    "    # Load the audio file.\n",
    "    rawsound = AudioSegment.from_file(WAVE_OUTPUT_FILENAME_TESTING) \n",
    "    # Normalize the audio to +5.0 dBFS.\n",
    "    normalizedsound = effects.normalize(rawsound, headroom = 0) \n",
    "    # Transform the normalized audio to np.array of samples.\n",
    "    normal_x = np.array(normalizedsound.get_array_of_samples(), dtype = 'float32')\n",
    "    # Trim silence from the beginning and the end.\n",
    "    xt, index = librosa.effects.trim(normal_x, top_db=30)\n",
    "      #print(file,\"\\t\", len(xt), \"\\t\", rawsound.dBFS, \"\\t\", normalizedsound.dBFS) #--QA purposes if needed-- \n",
    "    # Pad for duration equalization.\n",
    "    padded_x = np.pad(xt, (0, 204288-len(xt)), 'constant')\n",
    "    # Noise reduction.\n",
    "    final_x = nr.reduce_noise(y=padded_x,\n",
    "                          sr=sr)\n",
    " \n",
    "    # Features extraction \n",
    "    f1 = librosa.feature.rms(y=final_x, frame_length=frame_length, hop_length=hop_length) # Energy - Root Mean Square   \n",
    "    f2 = librosa.feature.zero_crossing_rate(y=final_x , frame_length=frame_length, hop_length=hop_length, center=True) # ZCR      \n",
    "    f3 = librosa.feature.mfcc(y=final_x, sr=sr, n_mfcc=13, hop_length = hop_length) # MFCC\n",
    "      \n",
    "    # Filling the data lists  \n",
    "    rms_t.append(f1)\n",
    "    zcr_t.append(f2)\n",
    "    mfcc_t.append(f3)\n",
    "    \n",
    "    f_rms_t = np.asarray(rms_t).astype('float32')\n",
    "    f_rms_t = np.swapaxes(f_rms_t,1,2)\n",
    "    f_zcr_t = np.asarray(zcr_t).astype('float32')\n",
    "    f_zcr_t = np.swapaxes(f_zcr_t,1,2)\n",
    "    f_mfccs_t = np.asarray(mfcc_t).astype('float32')\n",
    "    f_mfccs_t = np.swapaxes(f_mfccs_t,1,2)\n",
    "\n",
    "    #print(file, emotionfix(name))\n",
    "\n",
    "\n",
    "    X = np.concatenate((f_zcr_t, f_rms_t, f_mfccs_t), axis=2)\n",
    "\n",
    "    predictions = model.predict(X, use_multiprocessing=True)\n",
    "\n",
    "    y_pred_class = np.argmax(predictions)\n",
    "    print(\"predictions : \" + str(y_pred_class)+ \" \" +emotions.get(y_pred_class))\n",
    "    \n",
    "    \n",
    "    pred_list = list(predictions)\n",
    "    pred_np = np.squeeze(np.array(pred_list).tolist(), axis=0) # Get rid of 'array' & 'dtype' statments.\n",
    "    total_predictions.append(pred_np)\n",
    "    print(pred_list)\n",
    "    fig = plt.figure(figsize = (10, 2))\n",
    "    plt.bar(emo_list, pred_np, color = 'darkturquoise')\n",
    "    plt.ylabel(\"Probabilty (%)\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481c5eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ABC = voice_predictions()\n",
    "ABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b27e592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836a530a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
